{"cells":[{"cell_type":"markdown","metadata":{"id":"tjbPl0NUtgAw"},"source":["#INTRODUCTION\n","The goal of this project is to create a model that can receive a text description and then select from a library of images the image that most closely aligns with that description.  The model architecture is a dual encoder that is trained to projects images and their descriptions onto the same space and at the same location.  To serve as the base models of the dual encoder, we will use Inception V3 to encode the image data and BERT to encode the text data.  On top of these base models will sit two small feedforward networks that will project the encoded images and text onto the common space where they will have the same dimensions.  Let us call these two feedforward networks the \"projection heads\".\n","\n","Because training for this dual encoder is so time consuming, we will speed up the process by breaking the training process into two phases.\n","##TRAINING PHASES\n","###Phase 1:\n","During phase 1, we will circumvent the base models completely, dramatically reducing the iteration time as we tune the model.  We will use the base models just once to encode the entire dataset, and then we will train the projection heads using just the pre-encoded data.\n","\n","###Phase 2:\n","During phase 2, we will use the original data to fine tune the entire dual encoder, including the base models.\n","\n","##Note\n","This project draws inspiration and some design features from the keras project \"Natural language image search with a Dual Encoder\", located at https://keras.io/examples/vision/nl_image_search/."]},{"cell_type":"markdown","source":["#Setup\n","We will use PyTorch to build the dual encoder, and we will use Google Colab to give us access to a GPU."],"metadata":{"id":"ecDdqTUgvsbg"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9DrYdIRsHUOZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718755090164,"user_tz":240,"elapsed":35119,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"0fe0a99a-f61f-4499-f426-dbbddf9c9a4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import zipfile\n","import requests\n","from tqdm import tqdm\n","import json\n","import shutil\n","import importlib.util\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from torchvision.io import read_image\n","from torchvision import transforms as torchvision_transforms\n","import json\n","from torch import nn\n","from torch.utils.data import DataLoader, random_split\n","if not importlib.util.find_spec('transformers'):\n","  !pip install transformers\n","from transformers import BertTokenizer, BertModel\n","from torchvision.models import inception_v3, Inception_V3_Weights\n","import h5py\n","from itertools import chain\n","import pickle\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Download the Data\n","We begin by downloading the MS COCO dataset, which contains both the image and caption data.  Note that the captions describe the images, and that they are stored in the \"annotations\" file.  The data is stored in zip files that we download for free."],"metadata":{"id":"Zrh3FYFGB0vW"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":849671,"status":"ok","timestamp":1718755939832,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"},"user_tz":240},"id":"U3rZ12_qkuqu","outputId":"62d86829-c813-4758-cb5b-055cae70520a"},"outputs":[{"output_type":"stream","name":"stderr","text":["data/train2014.zip: 100%|██████████| 12.6G/12.6G [06:54<00:00, 32.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracted all contents to data\n"]},{"output_type":"stream","name":"stderr","text":["data/val2014.zip: 100%|██████████| 6.19G/6.19G [03:14<00:00, 34.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracted all contents to data\n"]},{"output_type":"stream","name":"stderr","text":["data/annotations_trainval2014.zip: 100%|██████████| 241M/241M [00:07<00:00, 34.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracted all contents to data\n","MS-COCO dataset downloaded and extracted successfully!\n"]}],"source":["# Download the full MS-COCO dataset (unless using dev data)\n","\n","def download_and_extract(url):\n","        # Function that downloads and extracts files given a url\n","\n","        os.makedirs('data', exist_ok=True)\n","\n","        # Extract the filename from the URL\n","        filename = os.path.join('data', url.split(\"/\")[-1])\n","\n","        # Download the file if it doesn't exist\n","        if not os.path.exists(filename):\n","            response = requests.get(url, stream=True)\n","            total_size = int(response.headers.get('content-length', 0))\n","\n","            with open(filename, 'wb') as file, tqdm(\n","                desc=filename,\n","                total=total_size,\n","                unit='B',\n","                unit_scale=True,\n","                unit_divisor=1024,\n","            ) as bar:\n","                for data in response.iter_content(chunk_size=1024):\n","                    size = file.write(data)\n","                    bar.update(size)\n","\n","        # Extract the zip file\n","        with zipfile.ZipFile(filename, 'r') as zip_ref:\n","            zip_ref.extractall('data')\n","            print(f\"Extracted all contents to data\")\n","\n","def download_ms_coco():\n","    # Function to download the MS-COCO dataset (2017 version)\n","\n","    URLS = {\n","        \"train_images\": \"http://images.cocodataset.org/zips/train2014.zip\",\n","        \"val_images\": \"http://images.cocodataset.org/zips/val2014.zip\",\n","        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n","    }\n","\n","    # Download, extract images and annotations\n","    for url in URLS.values():\n","        download_and_extract(url)\n","\n","    print(\"MS-COCO dataset downloaded and extracted successfully!\")\n","\n","download_ms_coco()"]},{"cell_type":"markdown","source":["#Prepare the Data for Model Ingestion\n","We will generate PyTorch Datasets and Dataloaders using the original image and caption data.  Note that these datasets will only be used during phase 2 training.  However, we still need to create them now so that we can generate the pre-encoded datasets that will be used during phase 1."],"metadata":{"id":"L36olVHi_3Oq"}},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718755939833,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"},"user_tz":240},"id":"69Ew-0-8kq5X"},"outputs":[],"source":["# Prepare the image transforms\n","\n","class resize_and_pad_image:\n","    # Class to ensure that all images are the same size\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, image):\n","        h, w = image.size()[-2:]\n","        size_param = int(min(w/h, h/w) * self.size)\n","        image = torchvision_transforms.functional.resize(img=image, size=size_param)\n","        h_new, w_new = image.size()[-2:]\n","        dw = (self.size-w_new)//2\n","        dh = (self.size-h_new)//2\n","        rw = (self.size-w_new)%2\n","        rh = (self.size-h_new)%2\n","        return torchvision_transforms.functional.pad(image, padding=(dw, dh, dw+rw, dh+rh))\n","\n","image_transform = torchvision_transforms.Compose([\n","    resize_and_pad_image(size=299),\n","    torchvision_transforms.ConvertImageDtype(torch.float32),  # Convert to float tensor\n","    torchvision_transforms.Lambda(lambda x: x / 255),        # Scale pixel values\n","    torchvision_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# Define the images & captions dataset in PyTorch\n","\n","class image_and_captions_dateset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Class to provide a custom dataset for the images and captions\n","\n","    Attributes\n","    ----------\n","    annotations_file : str\n","        The path to the annotations file\n","    img_dir : str\n","        The path to the directory containing the images\n","    image_transform : torchvision.transforms.Compose\n","        The transform to apply to the images\n","\n","    Methods\n","    -------\n","    __len__()\n","        Returns the number of samples in the dataset\n","    __getitem__(idx)\n","        Returns the transformed image, the caption, and the sample Id at index idx\n","    \"\"\"\n","    def __init__(self, annotations_file, img_dir, image_transform):\n","\n","        with open(annotations_file, 'r') as f:\n","            self.annotations = json.load(f)['annotations']\n","        self.img_dir = img_dir\n","        self.image_transform = image_transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        sample_id = self.annotations[idx]['image_id']\n","        img_path = os.path.join(self.img_dir, 'COCO_train2014_%012d.jpg'%sample_id)\n","        image = read_image(img_path)\n","        if image.shape[0] == 1: image = image.tile((3,1,1))\n","        caption = self.annotations[idx]['caption']\n","        image = self.image_transform(image)\n","        return image, caption, sample_id"]},{"cell_type":"markdown","source":["##Create the PyTorch Datasets\n","Out dataset will contain transformed images and untransformed text captions.  The text will be tokenized during the dataloading process.\n","\n","Note that I limit that size of the training dataset because the full dataset is too large for my Google Cloud compute resources."],"metadata":{"id":"IlGVF36hEaaz"}},{"cell_type":"code","source":["annotations_file = \"./data/annotations/captions_train2014.json\"\n","img_dir = './data/train2014'\n","\n","# Generate the PyTorch dataset\n","coco_dataset = image_and_captions_dateset(annotations_file, img_dir, image_transform)\n","\n","\n","# Split the dataset.  We won't use the whole dataset because it is too large\n","splits = (0.4, 0.1, 0.1)\n","\n","train_size = int(splits[0] * len(coco_dataset))\n","val_size = int(splits[1] * len(coco_dataset))\n","test_size = int(splits[2] * len(coco_dataset))\n","extra = len(coco_dataset) - train_size - val_size - test_size\n","train_dataset, val_dataset, test_dataset, _ = random_split(coco_dataset, [train_size, val_size, test_size, extra])"],"metadata":{"id":"_81rS7SwEZoJ","executionInfo":{"status":"ok","timestamp":1718755941105,"user_tz":240,"elapsed":1278,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Create the PyTorch Dataloaders\n","\n","We tokenize during the dataloading process so that the length of the samples within each batch can be padded to the max sample length within the batch, rather than forcing all samples in all batches to be padded to the max sample length within the entire dataset.\n","\n","Since we are using BERT as our text encoder model, we must tokenize our captions using the BERT Tokenizer"],"metadata":{"id":"0fJ5CMuhEw6A"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"G_1ahEhB-Ktb","colab":{"base_uri":"https://localhost:8080/","height":308,"referenced_widgets":["c2198de044434623aae72116a6037506","04f9e1d4066248a0b19aefb78d35f6e1","ffc90d9126464535bfc02a3488c2b874","49388d2dbe014add894600f2319282a1","8c47a784100349f5bf1ec025f8e9f6de","0211a3023c45464980c1bd6fea1108d4","65ad8fe49d974931ab2038a3b99cd348","d25424e6bd4046d2a6f6e64d8b80a9c7","8645b97e46e94c17873314e9d76c52ae","12d5af7414a14d11a24c2ed303475343","58c7ce3d47e342aba49a893053ae3ec4","9fd36e71edd043f987c3e19ac41c4bf8","84c682403aa848b8903573c15b9337e4","c0d44fd7d6194eee8b07de284c9f2804","cdb5fdf1d9434d3c8aecd93994cc689f","2fb87371bd2746efb2a08812f90c43cc","445f3b79e902488d9545f68c9174470c","6b4e8887a01a4a3aaf72eb7857d9a942","16b399d812be4d37bb391922177767ab","a0a4d69ed94b4cbf95bc405e542d69b4","8648ddc3b82840f094beeca50c69fd17","c0cc4e38056141aea2b2b8dd9698cf00","508bc45358db4d8b97a57dc1673ada6c","909b8bc315c9472d998dcce27621f329","450de5a21fb840248c7eee03b5524e19","96581c3c8e9d4aeba344507fbecae49e","6006830ebfed4391afcf6de540d8fd9d","f6f7ca35055e430fb6fe13d862d63d93","7ad861a52bb144758460d8505d1add71","b24df9712dce4667aeda2798ae21730b","2938d406b3c146e2a67ea15811eb2061","923d19aa3fc149ee8af91ec65ecbb5a8","48ef19b66dd94ef49f8e208291e5ab27","9a5a70f40daa42ee98d53f932d14ebc7","1f88ecd65884458d988379642f110660","5430caf294bb43898fe3d52e8de2466b","4739a856521446daab14942dfa8d7824","96059a8f5c584221b651b204beec0def","48cb01c52e794750a9d0513b1c6a5fe7","cf69a5c233d6449e8b92d8c1a04a9635","f02f6f377e424d66a0541cf9aade41df","34dde43b65874b18ac0a3d985de9f66d","6e3e1504017f4ac5bb2e6e054c4ae2e0","37573b28775344d297a7e5ad191595c2"]},"executionInfo":{"status":"ok","timestamp":1718755944025,"user_tz":240,"elapsed":2925,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"911dca3f-da2c-41cb-b482-7127670d0171"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2198de044434623aae72116a6037506"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd36e71edd043f987c3e19ac41c4bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"508bc45358db4d8b97a57dc1673ada6c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5a70f40daa42ee98d53f932d14ebc7"}},"metadata":{}}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class collate_fn():\n","  \"\"\"\n","  Class to provide a custom collate function that modifies the batching process.\n","\n","  Attributes\n","  ----------\n","  tokenizer : BertTokenizer\n","      The tokenizer to use for tokenizing the captions.\n","\n","  Methods\n","  -------\n","  __call__(batch)\n","      Takes in a batch of captions and returns a batch of tokenized captions\n","      padded to the max caption length within the batch.\n","  \"\"\"\n","  def __init__(self, tokenizer):\n","    self.tokenizer = tokenizer\n","  def __call__(self, batch):\n","    [images, captions, sample_ids] = list(zip(*batch))\n","    embedded_captions = self.tokenizer(list(captions), padding='longest', truncation=True, return_tensors='pt')\n","    images = torch.stack(list(images))\n","    return (images, embedded_captions, captions, sample_ids)\n","\n","# Create DataLoader for each dataset\n","train_dataloader = DataLoader(train_dataset, batch_size = 16, collate_fn=collate_fn(tokenizer), shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size = 16, collate_fn=collate_fn(tokenizer), shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size = 16, collate_fn=collate_fn(tokenizer), shuffle=False)"]},{"cell_type":"markdown","source":["#Create the Elements of the Dual Encoder\n","The elements are:\n","\n","\n","1.   Base Model 1 - Inception V3 for image data\n","2.   Base Model 2 - BERT for text data\n","3.   Projection Head 1 for image data\n","4.   Projection Head 2 for text data\n","\n"],"metadata":{"id":"yWy9Q8ZTadc4"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"Qk8js4gyiynn","colab":{"base_uri":"https://localhost:8080/","height":228,"referenced_widgets":["233370a9391f4b75b820d41a15860a18","91d0eb387288433daee736b2878cadb8","52a2b0b35b8b44deb6518d2de658580d","b621c13ae69940e69577d47813192354","a8a02121a78d4fc1bc66ad7b49a49f31","b004cea1372945d5a0862911325c06c3","1c0dcdcaa0954e2f843f62078ccc1331","9d630d5b17ed4f2e967ac0115599d41b","50a33e0eb4554bb3b2be114ce0462f25","60078a511c764ca7b7806175db96687e","19509dbe7dd24f4d916f259bf94b2adc"]},"executionInfo":{"status":"ok","timestamp":1718755952684,"user_tz":240,"elapsed":8672,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"a527e997-e8b6-4bd9-a491-ec76baf4439d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n","100%|██████████| 104M/104M [00:02<00:00, 46.5MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233370a9391f4b75b820d41a15860a18"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["projection_head(\n","  (lin1): Linear(in_features=768, out_features=256, bias=True)\n","  (gelu): GELU(approximate='none')\n","  (layernorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (lin2): Linear(in_features=256, out_features=256, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (layernorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":6}],"source":["# Define the Projection Head class that will be used to transform the outputs of both the text and image models into the same embedding space.\n","\n","class projection_head(nn.Module):\n","    \"\"\"\n","    Module to provide the projection head that will be used to project the outputs of both the text and image models into the same embedding space.\n","\n","    Attributes\n","    ----------\n","    in_features : int\n","        The number of features outputted by the base model.\n","    projection_dims : int\n","        The number of features in the text/image common embedding space.\n","    dropout_rate : float\n","        The dropout rate to use\n","    \"\"\"\n","    def __init__(self, in_features, projection_dims, dropout_rate):\n","        super().__init__()\n","        self.lin1 = nn.Linear(in_features, projection_dims)\n","        self.gelu = nn.GELU()\n","        self.layernorm1 = nn.LayerNorm(projection_dims)\n","        self.lin2 = nn.Linear(projection_dims, projection_dims)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.layernorm2 = nn.LayerNorm(projection_dims)\n","    def forward(self, x):\n","        x1 = self.lin1(x)\n","        x = self.gelu(x1)\n","        x = self.layernorm1(x)\n","        x = self.lin2(x)\n","        x = self.dropout(x)\n","        x = self.layernorm2(x)\n","        return x\n","\n","\n","# Load in the BERT model and the Inception model, and create projection heads for each\n","\n","image_embedder = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n","image_projection_head = projection_head(in_features=image_embedder.fc.in_features, projection_dims=256, dropout_rate=0.1)\n","image_AuxLogits_projection_head = projection_head(in_features=image_embedder.AuxLogits.fc.in_features, projection_dims=256, dropout_rate=0.1)\n","image_embedder.fc = torch.nn.Identity()\n","image_embedder.AuxLogits.fc = torch.nn.Identity()\n","caption_embedder = BertModel.from_pretrained('bert-base-uncased')\n","caption_projection_head = projection_head(in_features=caption_embedder.pooler.dense.out_features, projection_dims=256, dropout_rate=0.1)\n","\n","# Move all elements of the dual encoder to the GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","image_embedder.to(device)\n","caption_embedder.to(device)\n","image_projection_head.to(device)\n","caption_projection_head.to(device)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"xUSf9JTWnBPB"}},{"cell_type":"markdown","source":["#Generate the Base Embeddings for Phase 1\n","Now, we use our base models to pre-embed both our images and captions.  Both models will be run in inference mode.  Later, we will use these base-embeddings to create the dataset that we will use to train our projection heads during Phase 1 of training.  We will generate both a file containing the embeddings and a file containing the original captions and sample IDs.  This second file will allow us to map the base-embeddings to the original dataset, which we need to do in order to evaluate our model performance after Phase 1 training."],"metadata":{"id":"LHNvG3qznG7h"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"f9zW8gGQ2QJj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718756098116,"user_tz":240,"elapsed":145444,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"876972a9-c493-4b07-b602-88b7c2fa7855"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-e187d5288db2>:69: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  image_embeddings = torch.tensor(data['image_embeddings'], dtype=torch.float)\n"]}],"source":["def generate_base_embeddings(image_embedder, text_embedder, dataloader, base_embeddings_file, base_model_embeddings_metadata_file):\n","    \"\"\"\n","    Function to generate and save the base embeddings for the entire dataset.\n","    The embeddings are collected in two lists and then saved into an HDF5 file and a pickle file.\n","\n","    Inputs\n","    -------\n","    image_embedder : torch.nn.Module\n","        The image model to use for embedding the images.\n","    text_embedder : torch.nn.Module\n","        The text model to use for embedding the captions.\n","    dataloader : torch.utils.data.DataLoader\n","        The dataloader to use for generating the original dataset's data.\n","    base_embeddings_file : str\n","        The path to the file to store the base embeddings.\n","    base_model_embeddings_metadata_file : str\n","        The path to the file to store the original captions and sample ids.\n","\n","    Outputs\n","    -------\n","    h5py file is stored at base_embeddings_file\n","    \"\"\"\n","\n","    image_embedder.eval(); text_embedder.eval()\n","\n","    with torch.no_grad():\n","\n","        # Create a temporary file to store the embeddings\n","        with h5py.File('temporary_file.h5', 'w') as h5f:\n","\n","            # Create a dataset within the HDF5 file to store embeddings\n","            image_embeddings_h5 = h5f.create_dataset(\"image_embeddings\", (len(dataloader.dataset), 2048), dtype='float32')\n","            caption_embeddings_h5 = h5f.create_dataset(\"caption_embeddings\", (len(dataloader.dataset), 768), dtype='float32')\n","\n","            # Lists to store the data as it is embedded in batches\n","            caption_list = []\n","            sample_id_list = []\n","\n","            for batch, (images, tokenized_captions, captions, sample_ids) in enumerate(tqdm(dataloader, desc='Embedding Data')):\n","                images = images.to(device)\n","                for item in tokenized_captions:\n","                  tokenized_captions[item] = tokenized_captions[item].to(device)\n","                images_out = image_embedder(images)\n","                captions_out = caption_embedder(**tokenized_captions).pooler_output\n","                image_embeddings_h5[batch*16:(batch*16+len(images_out))] = images_out.cpu().numpy()\n","                caption_embeddings_h5[batch*16:(batch*16+len(images_out))] = captions_out.cpu().numpy()\n","                caption_list = caption_list + list(captions)\n","                sample_id_list = sample_id_list + list(sample_ids)\n","\n","    !mv temporary_file.h5 {base_embeddings_file}\n","\n","    with open('temporary_file.pkl', \"wb\") as file:\n","        pickle.dump((caption_list, sample_id_list), file)\n","    !mv temporary_file.pkl {base_model_embeddings_metadata_file}\n","\n","\n","# Establish the target location at which to save the base-embeddings\n","\n","os.makedirs('/content/drive/My Drive/Projects/text-to-image-search/data', exist_ok=True)\n","base_embeddings_file = '/content/drive/My Drive/Projects/text-to-image-search/data/base_embeddings.h5'\n","base_model_embeddings_metadata_file = \"/content/drive/My Drive/Projects/text-to-image-search/data/base_embeddings_metadata.pkl\"\n","\n","# Generate the base-embeddings\n","if not os.path.isfile(base_embeddings_file):\n","      generate_base_embeddings(image_embedder, caption_embedder, train_dataloader, base_embeddings_file, base_model_embeddings_metadata_file)\n","\n","# Load the base-embeddings, as well as the captions and sample IDs\n","data = h5py.File(base_embeddings_file, 'r')\n","image_embeddings = torch.tensor(data['image_embeddings'], dtype=torch.float)\n","caption_embeddings = torch.tensor(data['caption_embeddings'], dtype=torch.float)\n","\n","with open(base_model_embeddings_metadata_file, \"rb\") as file:\n","    (caption_list, sample_id_list) = pickle.load(file)"]},{"cell_type":"markdown","source":["#Generate the Phase 1 Training Dataset\n","Now that we have the base-embeddings, we can use them to create a dataset that we will use for Phase 1 training of just the projection heads.  We will also split off some of the base-embeddings to use in a validation dataset."],"metadata":{"id":"-2_fMb37vLhd"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"-7SLa6Mp-dDM","executionInfo":{"status":"ok","timestamp":1718756098116,"user_tz":240,"elapsed":18,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"outputs":[],"source":["# Generate the dataset for the pre-embedded data\n","\n","class Embeddings_Dataset(torch.utils.data.Dataset):\n","    def __init__(self, image_embeddings, caption_embeddings, caption, sample_id):\n","\n","        self.image_embeddings = image_embeddings\n","        self.caption_embeddings = caption_embeddings\n","        self.caption = caption\n","        self.sample_id = sample_id\n","\n","    def __len__(self):\n","        return len(self.caption_embeddings)\n","\n","    def __getitem__(self, idx):\n","        return self.image_embeddings[idx], self.caption_embeddings[idx], self.caption[idx], self.sample_id[idx]\n","\n","\n","validation_cutoff = round(len(image_embeddings)*0.05)\n","embeddings_train_dataset = Embeddings_Dataset(image_embeddings[:-validation_cutoff], caption_embeddings[:-validation_cutoff], caption_list[:-validation_cutoff], sample_id_list[:-validation_cutoff])\n","embeddings_train_dataloader = DataLoader(embeddings_train_dataset, batch_size=16, shuffle=True)\n","embeddings_val_dataset = Embeddings_Dataset(image_embeddings[-validation_cutoff:], caption_embeddings[-validation_cutoff:], caption_list[-validation_cutoff:], sample_id_list[-validation_cutoff:])\n","embeddings_val_dataloader = DataLoader(embeddings_val_dataset, batch_size=16, shuffle=True)"]},{"cell_type":"markdown","source":["# Functions and Classes for Phase 1 Training\n","Before we begin Phase 1 training of the projection heads, we need to create the training functions and define the loss function.  We use a custom loss function that allows us to compare all potential pairings and images and captions within an individual batch. Because our projection heads output a layer-normalized vector for each sample, we can simply use a matrix multiplication to check for projection similarity.  Matrix multiplication is advantageous because it is a quick and easy operation."],"metadata":{"id":"tvnH-AZJvxrw"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"2n3O2-AJSH9C","executionInfo":{"status":"ok","timestamp":1718756098116,"user_tz":240,"elapsed":17,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"outputs":[],"source":["# Create the custom loss class to utilize during training\n","\n","from torch import matmul\n","import torch.nn.functional as F\n","\n","class custom_loss(nn.Module):\n","    def forward(self, image_out, caption_out):\n","        predicted_sim = matmul(image_out, torch.transpose(caption_out, 0, 1))\n","        image_sim = matmul(image_out, torch.transpose(image_out, 0, 1))\n","        caption_sim = matmul(caption_out, torch.transpose(caption_out, 0, 1))\n","        targets = F.softmax((image_sim+caption_sim)/2, dim=0)\n","        return (F.cross_entropy(predicted_sim, targets) + F.cross_entropy(torch.transpose(predicted_sim, 0, 1), targets)) / 2\n","\n","\n","# Fuctions for training\n","\n","def train_projection_head_one_epoch(image_embedder, text_embedder, dataloader, loss_fn, optimizer):\n","    image_embedder.train(); text_embedder.train()\n","    for batch, (images, captions, _, _) in enumerate(dataloader):\n","        images = images.to(device)\n","        captions = captions.to(device)\n","        images_out = image_embedder(images)\n","        captions_out = text_embedder(captions)\n","        loss = loss_fn(images_out, captions_out)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","def validate_projection_head(image_embedder, text_embedder, dataloader, loss_fn):\n","    image_embedder.eval(); text_embedder.eval()\n","    with torch.no_grad():\n","        val_loss = 0\n","        for batch, (images, captions, _, _) in enumerate(dataloader):\n","            images = images.to(device)\n","            captions = captions.to(device)\n","            images_out = image_embedder(images)\n","            captions_out = text_embedder(captions)\n","            val_loss += loss_fn(images_out, captions_out)\n","        return val_loss / len(dataloader)\n","\n","def train_projection_head(image_embedder, text_embedder, train_dataloader, val_dataloader, epochs, loss_fn, optimizer):\n","    for epoch in range(epochs):\n","        # print('Epoch', epoch)\n","        val_loss = validate_projection_head(image_embedder, text_embedder, val_dataloader, loss_fn)\n","        # print('Val_loss:', val_loss)\n","        train_projection_head_one_epoch(image_embedder, text_embedder, train_dataloader, loss_fn, optimizer)\n","        scheduler.step(val_loss)\n","\n","    return image_embedder, text_embedder"]},{"cell_type":"markdown","source":["# Phase 1 Training: Train the Projection Heads"],"metadata":{"id":"Xrq6xCwm0E-O"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718756098116,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"},"user_tz":240},"id":"NMtdp3octaQQ","outputId":"7eb036ce-a6bf-4c7c-f2ad-c51e2f5b0a53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved!\n"]}],"source":["# Train the projection heads\n","\n","image_projection_head.to(device)\n","caption_projection_head.to(device)\n","\n","loss_fn = custom_loss()\n","optimizer = torch.optim.AdamW(chain(image_projection_head.parameters(), caption_projection_head.parameters()), lr=0.2, weight_decay=0.001)\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=2e-2, step_size_up=200, mode='triangular')\n","\n","image_projection_head, caption_projection_head = train_projection_head(image_projection_head,\n","                                                                       caption_projection_head,\n","                                                                       embeddings_train_dataloader,\n","                                                                       embeddings_val_dataloader,\n","                                                                       100,\n","                                                                       loss_fn,\n","                                                                       optimizer)\n","\n","os.makedirs('/content/drive/My Drive/Projects/text-to-image-search/models', exist_ok=True)\n","torch.save(caption_projection_head, '/content/drive/My Drive/Projects/text-to-image-search/models/caption_projection_head.pt')\n","torch.save(image_projection_head, '/content/drive/My Drive/Projects/text-to-image-search/models/image_projection_head.pt')\n","print('Saved!')"]},{"cell_type":"markdown","source":["## Load in the Projection Heads"],"metadata":{"id":"b0fxRDVo0z-S"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"H2gbK1905U3k","executionInfo":{"status":"ok","timestamp":1718756099264,"user_tz":240,"elapsed":1164,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"outputs":[],"source":["image_projection_head = torch.load('/content/drive/My Drive/Projects/text-to-image-search/models/image_projection_head.pt')\n","caption_projection_head = torch.load('/content/drive/My Drive/Projects/text-to-image-search/models/caption_projection_head.pt')"]},{"cell_type":"markdown","source":["# Evaluate the Performance\n","In order to evaluate the performance of Phase 1 training, we will generate projections for the entire validation dataset, and then check each projected caption to see which image projections are closest to it.  If the correct image is within the 5% of closest images, we will call it a success and count it towards our accuracy score.  Finally, we will print out some examples of image / caption pairs whose projections were closest to each other."],"metadata":{"id":"stMtQDP805LU"}},{"cell_type":"code","source":["def generate_projections(image_model, caption_model, dataloader):\n","    \"\"\"\n","    Function to generate (but not save) the projects for an entire dataset.\n","    The projections are collected in two large torch.Tensors, and projection metadata is collected into two lists.\n","\n","    Inputs\n","    -------\n","    image_projection_head : torch.nn.Module\n","        The projection head to use for projecting the images.\n","    caption_projection_head : torch.nn.Module\n","        The projection to use for projecting the captions.\n","    embeddings_val_dataloader : torch.utils.data.DataLoader\n","        The dataloader to use for generating the base-embeddings data.\n","\n","    Outputs\n","    -------\n","    image_proj_embeddings : torch.Tensor\n","        The Torch Tensor containing the image projections.\n","    caption_proj_embeddings : torch.Tensor\n","        The Torch Tensor containing the caption projections.\n","    captions_list : list\n","        The list of captions.\n","    sample_ids_list : list\n","        The list of sample ids.\n","    \"\"\"\n","\n","    image_model.to(device); caption_model.to(device)\n","    image_model.eval(); caption_model.eval()\n","    with torch.no_grad():\n","        image_proj_embeddings = torch.empty((len(dataloader.dataset), 256))\n","        caption_proj_embeddings = torch.empty((len(dataloader.dataset), 256))\n","        captions_list = []\n","        sample_ids_list = []\n","        for batch, (images_batch, captions_batch, captions, sample_ids) in enumerate(tqdm(dataloader, desc='Projecting Data')):\n","            images_batch = images_batch.to(device)\n","            if isinstance(captions_batch, dict):\n","                for item in captions_batch:\n","                  captions_batch[item] = captions_batch[item].to(device)\n","            else:\n","                captions_batch = captions_batch.to(device)\n","            image_proj_embeddings[batch*16:batch*16+len(images_batch), :] = image_model(images_batch)\n","            caption_proj_embeddings[batch*16:batch*16+len(images_batch), :] = caption_model(captions_batch)\n","            captions_list = captions_list + list(captions)\n","            sample_ids_list = sample_ids_list + list(sample_ids)\n","    return image_proj_embeddings, caption_proj_embeddings, captions_list, sample_ids_list\n","\n","\n","image_proj_embeddings, caption_proj_embeddings, captions_list, sample_ids_list = \\\n","    generate_projections(image_projection_head, caption_projection_head, embeddings_val_dataloader)\n","\n","\n","def performance_eval(image_proj_embeddings, caption_proj_embeddings, num_to_print, img_dir):\n","\n","    # Array to store the image that most closely aligns with each caption's projection\n","    selected_images = np.empty(len(caption_proj_embeddings)).astype('int')\n","\n","    k = round(len(image_proj_embeddings)*.05)\n","    counter = 0\n","    for i, caption in enumerate(caption_proj_embeddings):\n","      predicted_sim = matmul(caption, torch.transpose(image_proj_embeddings, 0, 1))\n","      selected_images[i] = torch.argmax(predicted_sim)\n","      if i in torch.topk(predicted_sim, k=k)[1]:\n","        counter += 1\n","\n","    print('Accuracy of', counter/len(caption_proj_embeddings))\n","\n","    # Print some captions and corresponding chosen images\n","    for i in range(num_to_print):\n","        print('Caption:', captions_list[i])\n","        print('Correct Image:', sample_ids_list[i])\n","        img_path = os.path.join(img_dir, 'COCO_train2014_%012d.jpg'%sample_ids_list[i])\n","        image = read_image(img_path)\n","        plt.imshow(image.permute(1, 2, 0)); plt.show()\n","        print('Selected Image:', sample_ids_list[selected_images[i]])\n","        selected_image_id = sample_ids_list[selected_images[i]]\n","        img_path = os.path.join(img_dir, 'COCO_train2014_%012d.jpg'%selected_image_id)\n","        image = read_image(img_path)\n","        plt.imshow(image.permute(1, 2, 0)); plt.show()\n","\n","performance_eval(image_proj_embeddings, caption_proj_embeddings, 5, img_dir)"],"metadata":{"id":"DuwWRXtu3Gce","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1emVZlJqGvKqZBbKq4ab7PY28a6fYuNGK"},"executionInfo":{"status":"ok","timestamp":1718758066721,"user_tz":240,"elapsed":17382,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"1625cb8f-4410-4491-d8cc-eae285aa443c"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["#Phase 1 Results: 50% Success Rate\n","While 50% might seem low, we can see from the selected images that the model has definitely learned a lot, and does connect some conceptual elements from the text and images.  There is still room for improvement, so let's see if we can fine tune the model and do even better."],"metadata":{"id":"eLln-CAt6fDB"}},{"cell_type":"markdown","metadata":{"id":"sfcE81UmOA2y"},"source":["# Phase 2: Fine Tune Base Models Using Original Dataset\n","At this point, we have projection heads that work reasonably well.  Now we can fine tune the base models and attempt to get even better performance.  We will build out the full dual encoder and continue training using the original datasets.  To create the image side the dual encoder, we will simply replace the top layer of the Inception V3 image_embedder with the image projection head.  To create the text side of the dual encoder, we will need to create a custom Module to extract the pooler outputs from the BERT model and feed them to the caption projection head."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"M2fIChrNyoNS","executionInfo":{"status":"ok","timestamp":1718758176554,"user_tz":240,"elapsed":644,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"outputs":[],"source":["# Create the image side of the dual encoder.  It will still be called the image_embedder.\n","\n","image_embedder.fc = image_projection_head\n","image_embedder.AuxLogits.fc = image_AuxLogits_projection_head\n","\n","\n","# Create the text side of the dual encoder.  We will call this the text_embedder\n","\n","class CaptionEmbedderWithProjectionHead(nn.Module):\n","  def __init__(self, caption_embedder, caption_projection_head):\n","    super().__init__()\n","    self.base_model = caption_embedder\n","    self.projection_head = caption_projection_head\n","\n","  def forward(self, x):\n","    base_model_output = self.base_model(**x)\n","    return self.projection_head(base_model_output.pooler_output)\n","\n","text_embedder = CaptionEmbedderWithProjectionHead(caption_embedder=caption_embedder, caption_projection_head=caption_projection_head)\n","\n"]},{"cell_type":"markdown","source":["## Functions for Phase 2 Training"],"metadata":{"id":"rBRJ9fSAwtGV"}},{"cell_type":"code","source":["def train_full_model_one_epoch(image_embedder, text_embedder, dataloader, loss_fn, optimizer):\n","    image_embedder.train(); text_embedder.train()\n","    for batch, (images, captions, _, _) in enumerate(tqdm(dataloader, desc='Epoch Progress')):\n","        images = images.to(device)\n","        for item in captions:\n","          captions[item] = captions[item].to(device)\n","        images_out = image_embedder(images)\n","        captions_out = text_embedder(captions)\n","        loss1 = loss_fn(images_out.logits, captions_out)\n","        loss2 = loss_fn(images_out.aux_logits, captions_out)\n","        loss = loss1 + 0.4 * loss2\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","def validate_full_model(image_embedder, text_embedder, dataloader, loss_fn):\n","    image_embedder.eval(); text_embedder.eval()\n","    with torch.no_grad():\n","        val_loss = 0\n","        for batch, (images, captions, _, _) in enumerate(tqdm(dataloader, desc='Calculating Val Loss')):\n","            images = images.to(device)\n","            for item in captions:\n","              captions[item] = captions[item].to(device)\n","            images_out = image_embedder(images)\n","            captions_out = text_embedder(captions)\n","            val_loss += loss_fn(images_out, captions_out)\n","        return val_loss / len(dataloader)\n","\n","def train_full_model(image_embedder, text_embedder, train_dataloader, val_dataloader, epochs, loss_fn, optimizer):\n","    for epoch in range(epochs):\n","        print('Epoch', epoch)\n","        val_loss = validate_full_model(image_embedder, text_embedder, val_dataloader, loss_fn)\n","        print('Val_loss:', val_loss)\n","        train_full_model_one_epoch(image_embedder, text_embedder, train_dataloader, loss_fn, optimizer)\n","        scheduler.step(val_loss)\n","    val_loss = validate_full_model(image_embedder, text_embedder, val_dataloader, loss_fn)\n","    print('Val_loss:', val_loss)\n","\n","    return image_embedder, text_embedder"],"metadata":{"id":"bSuNPsX4wjTB","executionInfo":{"status":"ok","timestamp":1718758225077,"user_tz":240,"elapsed":245,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["#Phase 2 Training: Fine Tune the Full Dual Encoder\n","Because the dual encoder is so large, training an epoch is very slow.  Therefore, we will just train for 1 epoch."],"metadata":{"id":"4DsW9X8u-W2_"}},{"cell_type":"code","execution_count":25,"metadata":{"id":"oWfLxoM4opRA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718762484320,"user_tz":240,"elapsed":4258991,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"33941154-76c7-498a-ba34-49120ed651b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["Calculating Val Loss: 100%|██████████| 2589/2589 [10:21<00:00,  4.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Val_loss: tensor(1.5703, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch Progress:   0%|          | 0/10353 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv2d(input, weight, bias, self.stride,\n","Epoch Progress: 100%|██████████| 10353/10353 [50:28<00:00,  3.42it/s]\n","Calculating Val Loss: 100%|██████████| 2589/2589 [10:08<00:00,  4.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Val_loss: tensor(0.9090, device='cuda:0')\n"]}],"source":["# Train the full models\n","\n","image_embedder.to(device)\n","text_embedder.to(device)\n","\n","loss_fn = custom_loss()\n","optimizer = torch.optim.AdamW(chain(image_embedder.parameters(), text_embedder.parameters()), lr=0.0001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n","\n","image_embedder, text_embedder = train_full_model(image_embedder, text_embedder, train_dataloader, val_dataloader, 1, loss_fn, optimizer)"]},{"cell_type":"code","source":["torch.save(image_embedder, '/content/drive/My Drive/Projects/text-to-image-search/models/image_embedder.pt')\n","torch.save(text_embedder, '/content/drive/My Drive/Projects/text-to-image-search/models/text_embedder.pt')\n","print('Saved!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yF3HotOoHVvl","executionInfo":{"status":"ok","timestamp":1718762688233,"user_tz":240,"elapsed":8321,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"93aa6b2f-ea2d-47b5-ac1b-f3e338628236"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved!\n"]}]},{"cell_type":"markdown","source":["# Evaluate Performance\n","We will be using the entire test dataset, which is 42K samples long, to evaluate performance."],"metadata":{"id":"CpAv5Qdg7zSp"}},{"cell_type":"code","execution_count":34,"metadata":{"id":"CR9JW-BLxVtH","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gGCZgYsmh4WrxA2x9-u6xx6G7JhFXLKy"},"executionInfo":{"status":"ok","timestamp":1718764659050,"user_tz":240,"elapsed":806996,"user":{"displayName":"Suraj Khetarpal","userId":"11336959052235020284"}},"outputId":"1d8d4f6a-fb0b-4b73-b976-68d19acb312c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["image_proj_embeddings, caption_proj_embeddings, captions_list, sample_ids_list = \\\n","    generate_projections(image_embedder, text_embedder, test_dataloader)\n","\n","performance_eval(image_proj_embeddings, caption_proj_embeddings, 5, img_dir)\n","\n","\n","\n","\n","\n","# selected_images = np.empty(len(caption_embeddings)).astype('int')\n","\n","# counter = 0\n","# for i, caption in enumerate(caption_embeddings):\n","#   # prediction_dist = torch.cdist(torch.unsqueeze(caption, 0), image_embeddings)[0]\n","#   # selected_images[i] = torch.argmin(prediction_dist)\n","#   predicted_sim = matmul(caption, torch.transpose(image_embeddings, 0, 1))\n","#   selected_images[i] = torch.argmax(predicted_sim)\n","#   if i in torch.topk(predicted_sim, k=10)[1]:\n","#     counter += 1\n","\n","# print('Accuracy of', counter/len(caption_embeddings))\n","\n","# for i in range(5):\n","#     print(captions[i])\n","#     selected_image_id = sample_ids[selected_images[i]]\n","#     img_path = os.path.join(img_dir, 'COCO_train2014_%012d.jpg'%selected_image_id)\n","#     image = read_image(img_path)\n","#     plt.imshow(image.permute(1, 2, 0)); plt.show()"]},{"cell_type":"markdown","source":["#Final Results: 78% Success Rate\n","Fine tuning pushed the success rate up from 50% to 78% in only 1 epoch of training.  Additional epochs may have driven that rate up to be higher.  Already, the dual encoder is doing a very good job of returning images that contain the correct objects.\n"],"metadata":{"id":"UOhDCtUgph9-"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[{"file_id":"1_pnwqDdCZtW7aCEUcKj_kagAzCBEJIfw","timestamp":1718418111973},{"file_id":"1tS9xLNH-yZZs67SJr4lvfPANVV32E0bo","timestamp":1713491604868},{"file_id":"1uZCFlWid5DseqjXPkwk7s91ptVSq6gLC","timestamp":1712595375095},{"file_id":"1xCtHIT7rNQveTD-p7e5PMK9nIlAeNPaZ","timestamp":1710624605719}],"authorship_tag":"ABX9TyMXiwZnUM7CSj8TnTedAQPU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c2198de044434623aae72116a6037506":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04f9e1d4066248a0b19aefb78d35f6e1","IPY_MODEL_ffc90d9126464535bfc02a3488c2b874","IPY_MODEL_49388d2dbe014add894600f2319282a1"],"layout":"IPY_MODEL_8c47a784100349f5bf1ec025f8e9f6de"}},"04f9e1d4066248a0b19aefb78d35f6e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0211a3023c45464980c1bd6fea1108d4","placeholder":"​","style":"IPY_MODEL_65ad8fe49d974931ab2038a3b99cd348","value":"tokenizer_config.json: 100%"}},"ffc90d9126464535bfc02a3488c2b874":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25424e6bd4046d2a6f6e64d8b80a9c7","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8645b97e46e94c17873314e9d76c52ae","value":48}},"49388d2dbe014add894600f2319282a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12d5af7414a14d11a24c2ed303475343","placeholder":"​","style":"IPY_MODEL_58c7ce3d47e342aba49a893053ae3ec4","value":" 48.0/48.0 [00:00&lt;00:00, 1.15kB/s]"}},"8c47a784100349f5bf1ec025f8e9f6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0211a3023c45464980c1bd6fea1108d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65ad8fe49d974931ab2038a3b99cd348":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d25424e6bd4046d2a6f6e64d8b80a9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8645b97e46e94c17873314e9d76c52ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12d5af7414a14d11a24c2ed303475343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58c7ce3d47e342aba49a893053ae3ec4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fd36e71edd043f987c3e19ac41c4bf8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84c682403aa848b8903573c15b9337e4","IPY_MODEL_c0d44fd7d6194eee8b07de284c9f2804","IPY_MODEL_cdb5fdf1d9434d3c8aecd93994cc689f"],"layout":"IPY_MODEL_2fb87371bd2746efb2a08812f90c43cc"}},"84c682403aa848b8903573c15b9337e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_445f3b79e902488d9545f68c9174470c","placeholder":"​","style":"IPY_MODEL_6b4e8887a01a4a3aaf72eb7857d9a942","value":"vocab.txt: 100%"}},"c0d44fd7d6194eee8b07de284c9f2804":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16b399d812be4d37bb391922177767ab","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0a4d69ed94b4cbf95bc405e542d69b4","value":231508}},"cdb5fdf1d9434d3c8aecd93994cc689f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8648ddc3b82840f094beeca50c69fd17","placeholder":"​","style":"IPY_MODEL_c0cc4e38056141aea2b2b8dd9698cf00","value":" 232k/232k [00:00&lt;00:00, 1.30MB/s]"}},"2fb87371bd2746efb2a08812f90c43cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"445f3b79e902488d9545f68c9174470c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4e8887a01a4a3aaf72eb7857d9a942":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16b399d812be4d37bb391922177767ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0a4d69ed94b4cbf95bc405e542d69b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8648ddc3b82840f094beeca50c69fd17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0cc4e38056141aea2b2b8dd9698cf00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"508bc45358db4d8b97a57dc1673ada6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_909b8bc315c9472d998dcce27621f329","IPY_MODEL_450de5a21fb840248c7eee03b5524e19","IPY_MODEL_96581c3c8e9d4aeba344507fbecae49e"],"layout":"IPY_MODEL_6006830ebfed4391afcf6de540d8fd9d"}},"909b8bc315c9472d998dcce27621f329":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6f7ca35055e430fb6fe13d862d63d93","placeholder":"​","style":"IPY_MODEL_7ad861a52bb144758460d8505d1add71","value":"tokenizer.json: 100%"}},"450de5a21fb840248c7eee03b5524e19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b24df9712dce4667aeda2798ae21730b","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2938d406b3c146e2a67ea15811eb2061","value":466062}},"96581c3c8e9d4aeba344507fbecae49e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_923d19aa3fc149ee8af91ec65ecbb5a8","placeholder":"​","style":"IPY_MODEL_48ef19b66dd94ef49f8e208291e5ab27","value":" 466k/466k [00:00&lt;00:00, 1.91MB/s]"}},"6006830ebfed4391afcf6de540d8fd9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f7ca35055e430fb6fe13d862d63d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad861a52bb144758460d8505d1add71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b24df9712dce4667aeda2798ae21730b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2938d406b3c146e2a67ea15811eb2061":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"923d19aa3fc149ee8af91ec65ecbb5a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48ef19b66dd94ef49f8e208291e5ab27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a5a70f40daa42ee98d53f932d14ebc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f88ecd65884458d988379642f110660","IPY_MODEL_5430caf294bb43898fe3d52e8de2466b","IPY_MODEL_4739a856521446daab14942dfa8d7824"],"layout":"IPY_MODEL_96059a8f5c584221b651b204beec0def"}},"1f88ecd65884458d988379642f110660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48cb01c52e794750a9d0513b1c6a5fe7","placeholder":"​","style":"IPY_MODEL_cf69a5c233d6449e8b92d8c1a04a9635","value":"config.json: 100%"}},"5430caf294bb43898fe3d52e8de2466b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f02f6f377e424d66a0541cf9aade41df","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34dde43b65874b18ac0a3d985de9f66d","value":570}},"4739a856521446daab14942dfa8d7824":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e3e1504017f4ac5bb2e6e054c4ae2e0","placeholder":"​","style":"IPY_MODEL_37573b28775344d297a7e5ad191595c2","value":" 570/570 [00:00&lt;00:00, 18.5kB/s]"}},"96059a8f5c584221b651b204beec0def":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48cb01c52e794750a9d0513b1c6a5fe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf69a5c233d6449e8b92d8c1a04a9635":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f02f6f377e424d66a0541cf9aade41df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34dde43b65874b18ac0a3d985de9f66d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e3e1504017f4ac5bb2e6e054c4ae2e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37573b28775344d297a7e5ad191595c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"233370a9391f4b75b820d41a15860a18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91d0eb387288433daee736b2878cadb8","IPY_MODEL_52a2b0b35b8b44deb6518d2de658580d","IPY_MODEL_b621c13ae69940e69577d47813192354"],"layout":"IPY_MODEL_a8a02121a78d4fc1bc66ad7b49a49f31"}},"91d0eb387288433daee736b2878cadb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b004cea1372945d5a0862911325c06c3","placeholder":"​","style":"IPY_MODEL_1c0dcdcaa0954e2f843f62078ccc1331","value":"model.safetensors: 100%"}},"52a2b0b35b8b44deb6518d2de658580d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d630d5b17ed4f2e967ac0115599d41b","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50a33e0eb4554bb3b2be114ce0462f25","value":440449768}},"b621c13ae69940e69577d47813192354":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60078a511c764ca7b7806175db96687e","placeholder":"​","style":"IPY_MODEL_19509dbe7dd24f4d916f259bf94b2adc","value":" 440M/440M [00:03&lt;00:00, 129MB/s]"}},"a8a02121a78d4fc1bc66ad7b49a49f31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b004cea1372945d5a0862911325c06c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c0dcdcaa0954e2f843f62078ccc1331":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d630d5b17ed4f2e967ac0115599d41b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50a33e0eb4554bb3b2be114ce0462f25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60078a511c764ca7b7806175db96687e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19509dbe7dd24f4d916f259bf94b2adc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}